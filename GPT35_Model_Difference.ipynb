{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of ChatGPT 3.5 Models Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this small experiment is to use the same set of instructions and compare the answers provided by different ChatGPT 3.5 models.\n",
    "\n",
    "The answers are limited to the content of a set of documents, therefore, a RAG approach is used.\n",
    "\n",
    "Created by: Jhonnatan Torres\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stored my OpenAI key in a .env file, as a result, if you want to reproduce this notebook, you would need to install `pip install python-dotenv`, create a `.env` file in the same directory than the notebook and add your key to the `OPENAI_API_KEY` entry in the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models to be used in this small experiment are referenced in the official documentation of OpenAI, https://platform.openai.com/docs/models/gpt-3-5-turbo, these are part of the \"3.5\" family and work with a 16K context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-3.5-turbo-0125\", \"gpt-3.5-turbo-1106\", \"gpt-3.5-turbo-16k-0613\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the set of instructions, simple RAG application, with some follow-up capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FIRST_MESSAGE = '''You will be asked a series of questions or issues which can be found between \n",
    "the <QUESTION> XML Tags.\n",
    "A collection of documents that must be used to provide an answer to 'QUESTION' can be found \n",
    "between the <SOURCES> XML Tags.\n",
    "\n",
    "Use your experience as an AI Assistant, troubleshoot and respond with an answer to \n",
    "each 'QUESTION' following these guidelines:\n",
    "\n",
    "- Limit your knowledge to 'SOURCES' and determine if its content can provide a full and honest answer to 'QUESTION', \n",
    "  if it does, then respond with a honest answer. \n",
    "- Limit your answer to the content of the 'SOURCES'.  \n",
    "- Don't include more information or your chain of thought in the answer.\n",
    "- If you are unsure about the answer, or 'QUESTION' is not clear, or it is an  open question, then feel free \n",
    "  to respond with a follow-up question for the student to get a better understanding of the 'QUESTION' and \n",
    "  keep the troubleshooting.\n",
    "- If the content in the 'SOURCES' cannot provide a complete and honest answer to 'QUESTION' \n",
    "  then respond with \"IDK\".'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the sources that should be used to provide an Answer (*mix of real and made up documents*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES='''\n",
    "DOC1234: In python a lambda function can be created following this structure ```lambda x: x**2```.\n",
    "DOC1235: Collection from the numbutils can be really handy to get the value counts of items in a list in python.\n",
    "DOC1236: You can elevate a number to an `x` power by using the `**``character in python, \n",
    "for example ```x ** 2** is equal to `x to the power of 2`.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## First Use Case:\n",
    "The question entered by the student is not clear, therefore, the expected answer that should be provided by the chatbot is a follow-up question for the student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model[gpt-3.5-turbo-0125]:\n",
      "IDK\n",
      "Model[gpt-3.5-turbo-1106]:\n",
      "IDK\n",
      "Model[gpt-3.5-turbo-16k-0613]:\n",
      "I'm sorry, but the content in the sources does not provide a complete answer to your question. Could you please clarify what you are trying to do?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "for m in models:\n",
    "  response = client.chat.completions.create(\n",
    "    model = m,\n",
    "    messages = [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly AI Assistant helping to students who have a basic knowledge \\\n",
    "          about programming languages\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": USER_FIRST_MESSAGE\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"OK. Instructions Are Clear.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"<QUESTION>How can I create a lambda function in Python?</QUESTION>\\n<SOURCES>\\{SOURCES}</SOURCES>\\nOutput:\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"To create a lambda function in Python, you can use the following structure: `lambda x: x**2.`\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"<QUESTION>How can I do this?</QUESTION>\\n<SOURCES>\\n{SOURCES}\\n</SOURCES>\\nOutput:\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    max_tokens=50,\n",
    "    top_p=0.1,\n",
    "  )\n",
    "  print(f\"Model[{m}]:\")\n",
    "  print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the official documentation of OpenAI (https://platform.openai.com/docs/models/gpt-3-5-turbo) the `gpt-3.5-turbo-0125` is the most recent or updated model, and `gpt-3.5-turbo-16k-0613` is in a legacy status:\n",
    "> gpt-3.5-turbo-16k-0613\n",
    "\n",
    "> \"Legacy - Snapshot of gpt-3.5-16k-turbo from June 13th 2023. Will be deprecated on June 13, 2024.\"\n",
    "\n",
    "However, the model which is in a \"legacy\" status, was able to answer with the (*expected*) follow-up question for the student.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Use Case:\n",
    "Providing an answer to the question entered by the student based on the sources only. (RAG Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model[gpt-3.5-turbo-0125]:\n",
      "Collection from the numbutils can be really handy to get the value counts of items in a list in Python.\n",
      "Model[gpt-3.5-turbo-1106]:\n",
      "To get the value counts of items in a list in Python, you can use the collection from the `numbutils`.\n",
      "Model[gpt-3.5-turbo-16k-0613]:\n",
      "To get the value counts of items in a list in Python, you can use the `collections` module from the `numbutils` library.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "for m in models:\n",
    "  response = client.chat.completions.create(\n",
    "    model = m,\n",
    "    messages = [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly AI Assistant helping to students who have a basic knowledge \\\n",
    "          about programming languages\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": USER_FIRST_MESSAGE\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"OK. Instructions Are Clear.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"<QUESTION>How can I create a lambda function in Python?</QUESTION>\\n<SOURCES>\\{SOURCES}</SOURCES>\\nOutput:\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"To create a lambda function in Python, you can use the following structure: `lambda x: x**2.`\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"<QUESTION>How can I get the value counts of items in a list in python?</QUESTION>\\n<SOURCES>\\n{SOURCES}\\n</SOURCES>\\nOutput:\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    max_tokens=50,\n",
    "    top_p=0.1,\n",
    "  )\n",
    "  print(f\"Model[{m}]:\")\n",
    "  print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know, it can be a little subjective, but the answer provided by `gpt-3.5-turbo-16k-0613` looks better, or at least, more friendly, but it assumed `numbutils` was a library, and that fact was not mentioned in the sources.\n",
    "\n",
    "The answer provided by `gpt-3.5-turbo-1106` is friendly and it is not adding information outside of the sources.\n",
    "\n",
    "`gpt-3.5-turbo-0125` provided the exact text referenced in the sources: *\"Collection from the numbutils can be really handy to get the value counts of items in a list in Python\"*. It did not alter the word order or attempt to rephrase it to answer the question.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I know, it is not possible to draw conclusions with these simple experiments, however, my recommendation is to keep in mind these differences in the answers and the behaviour of the models when designing your RAG applications.\n",
    "\n",
    "- To be honest, I was expecting `gpt-3.5-turbo-0125` was going to be able to reply with a follow-up question for the student in the first use case, but in contrast, the 'legacy' model performed better in following the instructions.\n",
    "\n",
    "- Note: I included the instructions in the first user message because I wanted to reduce the number of tokens used in each turn, this way, I am not sending the Question, Sources and Instructions in each user turn. *I know, it is not the common RAG approach*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
